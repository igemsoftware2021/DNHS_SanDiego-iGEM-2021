{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb20634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a59c66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30408916",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removed unnecessary columns\n",
    "final_data = data.drop(data.columns[2:5], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "521c2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into X, the inputs (table of VOC values), and Y, the output (Risk- High or Low)\n",
    "\n",
    "X = final_data.iloc[:, 2:].copy()\n",
    "Y = final_data.iloc[:, 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9b5da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing \"High\" with 1 and \"Low\" with 0 in order to later use classification algorithms\n",
    "\n",
    "for x in range(0, len(Y)):\n",
    "    if Y[x] == \"High\":\n",
    "        Y[x] = 1\n",
    "    else:\n",
    "        Y[x] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3fa02b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train: input datframe for training the classifcation models\n",
    "# X_test: input dataframe for testing the classification models\n",
    "# Y_train: output datframe for training the classification models\n",
    "# Y_test: output datframe for testing the classification models\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dac7e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting output from \"object\" to int\n",
    "\n",
    "Y_train = Y_train.astype(\"int\")\n",
    "Y_test = Y_test.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0174e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary containing classifiers that use different algorithms. \n",
    "#We will iterate through this dictionary to find the most optimal classifier.\n",
    "#keys are names, values are instances of the classifiers\n",
    "\n",
    "dict_classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"Linear SVM\": SVC(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(n_estimators=1000),\n",
    "    \"Decision Tree\": tree.DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=1000),\n",
    "    \"Neural Net\": MLPClassifier(alpha = 1),\n",
    "    \"Naive Bayes\": GaussianNB()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fc2728a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function that fits each classifier in dict_classifiers to X_train and Y_train\n",
    "#the time spent on fitting the classifier to the data is also recorded\n",
    "# test_score calculated for each classifier. This is the accuracy score found by using the model with X_test and Y_test\n",
    "#for each classifier, in the dictionary dict_models, an entry is added. \n",
    "#This entry is a dictionary containing the classifier, test_score, and time_spent\n",
    "#after iterating through dict_classifiers, dict_models is returned\n",
    "\n",
    "def batch_classify(X_train, Y_train, X_test, Y_test,verbose = True):\n",
    "  \n",
    "    \n",
    "    dict_models = {}\n",
    "    \n",
    "    for classifier_name, classifier in list(dict_classifiers.items()):\n",
    "        t_start = time.perf_counter()\n",
    "        classifier.fit(X_train, Y_train)\n",
    "        t_end = time.perf_counter()\n",
    "\n",
    "        time_spent = t_end - t_start\n",
    "        test_score = classifier.score(X_test, Y_test)\n",
    "\n",
    "        dict_models[classifier_name] = {'model': classifier, 'test_score': test_score, 'train_time': time_spent}\n",
    "\n",
    "    return dict_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f962fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to display the classifiers' results in a table, sorted by test_score\n",
    "#lists made of the classifiers, their corresponding test_scores, and their corresponding training_time\n",
    "\n",
    "\n",
    "def display_dict_models(dict_models, sort_by='test_score'):\n",
    "    classifier_list = [x for x in dict_models.keys()]\n",
    "    test_score = [dict_models[key]['test_score'] for key in classifier_list]\n",
    "    training_time = [dict_models[key]['train_time'] for key in classifier_list]\n",
    "    \n",
    "    #initalized dataframe with correct dimensions and column labels\n",
    "    table = pd.DataFrame(data=np.zeros(shape=(len(classifier_list),3)), columns = ['classifier', 'test_score', 'train_time'])\n",
    "    \n",
    "    #for loop to set the values in each row\n",
    "    for x in range(0,len(classifier_list)):\n",
    "        table.loc[x, 'classifier'] = classifier_list[x]\n",
    "        table.loc[x, 'test_score'] = test_score[x]\n",
    "        table.loc[x, 'train_time'] = training_time[x]\n",
    "    \n",
    "    #function displays the table sorted by test_score, in descending order\n",
    "    display(table.sort_values(by=sort_by, ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c37a42ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def multiple:\n",
    " #   for x in range(10):\n",
    "   # X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)\n",
    "  #  Y_train = Y_train.astype(\"int\")\n",
    "    #Y_test = Y_test.astype(\"int\")\n",
    "    \n",
    "    #batch_classify(X_train, Y_train, X_test, Y_test,verbose = True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c6ecf45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yajat\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Neural Net</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.114065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.002139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.003241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>1.011021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.031241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nearest Neighbors</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.004719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.004653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.804293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     classifier  test_score  train_time\n",
       "6                    Neural Net    0.684211    0.114065\n",
       "4                 Decision Tree    0.631579    0.002139\n",
       "7                   Naive Bayes    0.631579    0.003241\n",
       "5                 Random Forest    0.578947    1.011021\n",
       "0           Logistic Regression    0.526316    0.031241\n",
       "1             Nearest Neighbors    0.526316    0.004719\n",
       "2                    Linear SVM    0.473684    0.004653\n",
       "3  Gradient Boosting Classifier    0.421053    0.804293"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dict_models = batch_classify(X_train, Y_train, X_test, Y_test)\n",
    "display_dict_models(dict_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b8d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
